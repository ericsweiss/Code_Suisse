Our first design decision arised from the issue of parsing and storing the sample trade data. With over one million pieces of data, it was very important that we had the most efficient implementation possible. We made a HashMap, with a key of a string company symbol, with a value of a "Company" object. This way, we can look up companies in constant time. Our company object contains an array list of transactions (Transaction objects). We use an array list because the size of our array is unknown, but we still want to be able to index our list, iterate our list, and binary search our list. In order to look up a transaction in this array in constant time, we have a hashmap with a key of a date, that links to the integer index of the corresponding transaction that occured at that date. The reason why we needed the array list and not only a hashmap was so that we can easily iterate through the list through a given time frame. Given a time frame, we can lookup the starting point at constant time and easily iterate through. With the apparent issue that a time can be provided that was not a time that a trade occured, (key does not exist in the hashmap), we perform binary search on the list to find the closest previous time point a transaction occured in O(logn) time. 

Our second big decision came from how we classify companies in sectors. Since different companies had different amount of transactions, we had to choose a sample of transactions to determine if their trends are similar. In a side by side comparison of two companies, we picked the sample size as the amount of transactions of the company that has the lesser amount of transactions. We pick an even distribution of transactions over the time period for each company, and compared the percent change in price for each company. If the Pearsons correlation of the percent change over time between two companies is similar within our specified variable margin, we determine that the two companies are in the same sector. The setup for this comparison can be run in O(n^2) time, but that would require using the fewest number of datapoints. 
In order to increase accuracy, we decided to implement our algorithm in O(n^3) time, although it only takes about 20 seconds. This allows us to use far more datapoints, however, we still limit it to 2000 datapoints to avoid overfitting. 